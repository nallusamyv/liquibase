fastapi>=0.103.0
uvicorn>=0.23.0
streamlit>=1.28.0
langchain-core>=0.1.0
langchain-community>=0.0.10
langchain-openai>=0.0.5
chromadb>=0.4.0
snowflake-connector-python>=3.2.0
openai>=1.0.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
python-dotenv>=1.0.0
requests>=2.31.0
pandas>=2.0.0  # For data handling in Streamlit
tiktoken>=0.5.0  # For token counting
python-multipart>=0.0.6  # For file uploads in FastAPI

# config.py
import os
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    snowflake_user: str = os.getenv("SNOWFLAKE_USER")
    snowflake_password: str = os.getenv("SNOWFLAKE_PASSWORD")
    snowflake_account: str = os.getenv("SNOWFLAKE_ACCOUNT")
    snowflake_warehouse: str = os.getenv("SNOWFLAKE_WAREHOUSE")
    
    class Config:
        env_file = ".env"

settings = Settings()

# db_utils.py
import snowflake.connector
from config import settings

class SnowflakeManager:
    def __init__(self):
        self.conn = snowflake.connector.connect(
            user=settings.snowflake_user,
            password=settings.snowflake_password,
            account=settings.snowflake_account,
            warehouse=settings.snowflake_warehouse
        )
    
    def execute_query(self, sql: str) -> list:
        try:
            cur = self.conn.cursor()
            cur.execute(sql)
            return cur.fetchall()
        finally:
            cur.close()
    
    def __del__(self):
        self.conn.close()


# llm_handler.py
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from config import settings

class LLMQueryHandler:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(openai_api_key=settings.openai_api_key)
        self.llm = ChatOpenAI(
            temperature=0,
            openai_api_key=settings.openai_api_key
        )
        
        # Initialize VectorDB connections
        self.metadata_vdb = Chroma(
            persist_directory="./metadata_db",
            embedding_function=self.embeddings,
            collection_name="metadata"
        )
        
        self.query_vdb = Chroma(
            persist_directory="./query_db",
            embedding_function=self.embeddings,
            collection_name="verified_queries"
        )
    
    def generate_sql(self, nl_query: str) -> dict:
        # 1. Check cached queries
        similar = self.query_vdb.similarity_search(nl_query, k=1)
        if similar:
            return {
                "sql": similar[0].metadata['sql'],
                "source": "cache",
                "similarity_score": similar[0].metadata['score']
            }
        
        # 2. Retrieve relevant metadata
        context_docs = self.metadata_vdb.similarity_search(nl_query, k=5)
        context = "\n".join([d.page_content for d in context_docs])
        
        # 3. Generate SQL
        prompt = ChatPromptTemplate.from_messages([
            ("system", "Convert to SQL using:\n{context}"),
            ("human", "Query: {query}")
        ])
        
        chain = prompt | self.llm
        return {
            "sql": chain.invoke({"context": context, "query": nl_query}).content,
            "source": "generated"
        }
    
    def commit_to_knowledge(self, nl_query: str, sql: str, user_id: str) -> bool:
        try:
            self.query_vdb.add_texts(
                texts=[nl_query],
                metadatas=[{
                    "sql": sql,
                    "user": user_id,
                    "timestamp": str(datetime.now())
                }]
            )
            return True
        except Exception as e:
            raise RuntimeError(f"Commit failed: {str(e)}")


# api_server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from llm_handler import LLMQueryHandler
from db_utils import SnowflakeManager
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()
llm_handler = LLMQueryHandler()
snowflake = SnowflakeManager()

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request Models
class QueryRequest(BaseModel):
    natural_language: str
    user_id: str = "default"

class ExecutionRequest(BaseModel):
    sql: str

@app.post("/generate-sql")
async def generate_sql(request: QueryRequest):
    try:
        return llm_handler.generate_sql(request.natural_language)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/execute")
async def execute_query(request: ExecutionRequest):
    try:
        results = snowflake.execute_query(request.sql)
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/commit")
async def commit_query(request: QueryRequest, sql: str):
    try:
        success = llm_handler.commit_to_knowledge(
            request.natural_language,
            sql,
            request.user_id
        )
        return {"status": "success" if success else "failed"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


# Terminal 1 - Start API Server
python api_server.py

# Terminal 2 - Start UI
streamlit run salt_ui.py  # Use previous UI code
